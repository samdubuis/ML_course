{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "!pip3 install surprise\n",
    "from surprise import Reader\n",
    "from surprise import Dataset\n",
    "\n",
    "from surprise import SVD, KNNBasic, NMF, SlopeOne, CoClustering #other knn, randoms, not svdpp because were not doing implicit               # importer ici les algo qu'on testera\n",
    "from surprise import model_selection\n",
    "from surprise import dump\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Datasets/data_train.csv\")\n",
    "\n",
    "df[[\"user\", \"item\"]] = df.Id.str.split(\"_\", expand=True)\n",
    "\n",
    "df.user = df.user.str.replace(\"r\", \"\")\n",
    "df.item = df.item.str.replace(\"c\", \"\")\n",
    "\n",
    "#########\n",
    "df2 = pd.read_csv(\"Datasets/sample_submission.csv\")\n",
    "\n",
    "df2[[\"user\", \"item\"]] = df2.Id.str.split(\"_\", expand=True)\n",
    "\n",
    "df2.user = df2.user.str.replace(\"r\", \"\")\n",
    "df2.item = df2.item.str.replace(\"c\", \"\")\n",
    "prev_size=df.shape[0]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before re-running this cell, re-run the previous one. \n",
    "#If you are not happy with this cleaning set the two min_... var to 0. \n",
    "#Ok keep it to 0 for the moment -Alix 03/12/19. \n",
    "min_nb_of_users=0\n",
    "min_nb_of_items=0\n",
    "occur_user=df.user.value_counts()\n",
    "df_user_mod = df[~df.user.isin(occur_user[occur_user<=min_nb_of_users].index)]\n",
    "occur_item=df_user_mod.item.value_counts()\n",
    "df=df_user_mod[~df_user_mod.item.isin(occur_item[occur_item<=min_nb_of_items].index)]\n",
    "print(f\"Removed {prev_size-df.shape[0]} colunms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Prediction.value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création des datasets pour train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fifth, test = train_test_split(df, test_size=0.8, random_state=1)\n",
    "df_hundredth,test = train_test_split(df, test_size=0.99, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(1,5)) \n",
    "data = Dataset.load_from_df(df[[\"user\",\"item\",\"Prediction\"]], reader)\n",
    "data_fifth = Dataset.load_from_df(df_fifth[[\"user\",\"item\",\"Prediction\"]], reader)\n",
    "data_hundredth = Dataset.load_from_df(df_hundredth[[\"user\",\"item\",\"Prediction\"]], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_fifth, df_hundredth #freeing memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training de chaque algo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = SVD(n_factors=5, n_epochs=5000, lr_all=0.004, reg_all=0.07)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selection.cross_validate(algo, data, measures=[\"rmse\"], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo.fit(data.build_full_trainset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump.dump(\"dump/dump_SVD\", algo=algo, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# param_grid = {\n",
    "    'n_factors' : [5, 10],\n",
    "    'n_epochs': [20],\n",
    "    'lr_all': [0.002, 0.005],\n",
    "    'reg_all': [0.02, 0.03]\n",
    "} \n",
    "cv=5\n",
    "algorithm = SVD\n",
    "\n",
    "gs = model_selection.GridSearchCV(algorithm, param_grid, measures=['rmse'], cv=cv, n_jobs=-5, joblib_verbose=10)  #enlever mae car non utilisé dans le projet pour sauver du temps\n",
    "gs.fit(data) #checker quelle data est utilisée a chaque algo\n",
    "print(gs.best_params)\n",
    "print(gs.best_score)\n",
    "algo = gs.best_estimator[\"rmse\"]\n",
    "algo.fit(data.build_full_trainset())\n",
    "dump.dump(\"dump/SVDfitted_dump\", algo=algo, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'k' : [10, 40, 100],\n",
    "    'min_k': [1, 3, 5]\n",
    "} \n",
    "cv=5\n",
    "algorithm = KNNBasic\n",
    "\n",
    "gs = model_selection.GridSearchCV(algorithm, param_grid, measures=['rmse'], cv=cv, n_jobs=-7, joblib_verbose=10)  #enlever mae car non utilisé dans le projet pour sauver du temps\n",
    "gs.fit(data)\n",
    "print(gs.best_params)\n",
    "print(gs.best_score)\n",
    "algo = gs.best_estimator[\"rmse\"]\n",
    "algo.fit(data.build_full_trainset())\n",
    "dump.dump(\"dump/KNN_basic_fitted_dump\", algo=algo, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_factors' : [15, 50],\n",
    "    'n_epochs': [10, 50]\n",
    "} \n",
    "cv=5\n",
    "algorithm = NMF\n",
    "\n",
    "gs = model_selection.GridSearchCV(algorithm, param_grid, measures=['rmse'], cv=cv, n_jobs=-5, joblib_verbose=10)  #enlever mae car non utilisé dans le projet pour sauver du temps\n",
    "gs.fit(data)\n",
    "print(gs.best_params)\n",
    "print(gs.best_score)\n",
    "algo = gs.best_estimator[\"rmse\"]\n",
    "algo.fit(data.build_full_trainset())\n",
    "dump.dump(\"dump/NMFfitted_dump\", algo=algo, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SlopeOne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = SlopeOne()\n",
    "algo.fit(data.build_full_trainset())\n",
    "dump.dump(\"dump/SlopeOne_fitted_dump\", algo=algo, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CoClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_cltr_u' : [3, 5],\n",
    "    'n_cltr_i' : [3, 5], \n",
    "    'n_epochs': [50, 100]\n",
    "} \n",
    "cv=5\n",
    "algorithm = CoClustering\n",
    "\n",
    "gs = model_selection.GridSearchCV(algorithm, param_grid, measures=['rmse'], cv=cv, n_jobs=-5, joblib_verbose=10)  #enlever mae car non utilisé dans le projet pour sauver du temps\n",
    "gs.fit(data)\n",
    "print(gs.best_params)\n",
    "print(gs.best_score)\n",
    "algo = gs.best_estimator[\"rmse\"]\n",
    "algo.fit(data.build_full_trainset())\n",
    "dump.dump(\"dump/CoClustering_fitted_dump\", algo=algo, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading des pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del algo, gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_SVD = np.ones((df2.shape[0],1))\n",
    "array_KNN = np.ones((df2.shape[0],1))\n",
    "array_NMF = np.ones((df2.shape[0],1))\n",
    "array_SlopeOne = np.ones((df2.shape[0],1))\n",
    "array_CoClustering = np.ones((df2.shape[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, algo_svd = dump.load(\"dump/SVDfitted_dump\")\n",
    "_, algo_knn = dump.load(\"dump/KNN_basic_fitted_dump\")\n",
    "_, algo_nmf = dump.load(\"dump/NMFfitted_dump\")\n",
    "_, algo_slopeone = dump.load(\"dump/SlopeOne_fitted_dump\")\n",
    "_, algo_coclustering = dump.load(\"dump/CoClustering_fitted_dump\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "valid_items=df.item.unique()\n",
    "valid_users=df.user.unique()\n",
    "global_mean=df[\"Prediction\"].mean()\n",
    "count_bad_users=0\n",
    "count_bad_items=0\n",
    "count_bad_both=0\n",
    "global_mean=df[\"Prediction\"].mean()\n",
    "\n",
    "for i in df2.iterrows():\n",
    "    if i[0]%100000==0:\n",
    "        print(i[0])\n",
    "    if (i[1][\"user\"] in valid_users and i[1][\"item\"] in valid_items):\n",
    "        array_SVD[i[0]] = algo_svd.estimate(int(i[1][2])-1, int(i[1][3])-1)\n",
    "        array_KNN[i[0]]= algo_knn.estimate(int(i[1][2])-1, int(i[1][3])-1)[0]\n",
    "        array_NMF[i[0]] = algo_nmf.estimate(int(i[1][2])-1, int(i[1][3])-1)\n",
    "        array_SlopeOne[i[0]]=algo_SlopeOne.estimate(int(i[1][2])-1, int(i[1][3])-1)\n",
    "        array_CoClustering[i[0]] = algo_coclustering.estimate(int(i[1][2])-1, int(i[1][3])-1)\n",
    "    elif (i[1][\"user\"] in valid_users):\n",
    "        user_mean=df[df[\"user\"]==i[1][\"user\"]].mean()\n",
    "        array_SVD[i[0]] = user_mean\n",
    "        array_KNN[i[0]]= user_mean\n",
    "        array_NMF[i[0]] = user_mean\n",
    "        array_SlopeOne[i[0]]=user_mean\n",
    "        array_CoClustering[i[0]] = user_mean\n",
    "        count_bad_users+=1\n",
    "    elif (i[1][\"item\"] in valid_items):\n",
    "        item_mean=df[df[\"item\"]==i[1][\"item\"]].mean()\n",
    "        array_SVD[i[0]] = item_mean\n",
    "        array_KNN[i[0]]= item_mean\n",
    "        array_NMF[i[0]] = item_mean\n",
    "        array_SlopeOne[i[0]]= item_mean\n",
    "        array_CoClustering[i[0]] = item_mean\n",
    "        count_bad_items+=1\n",
    "    else:\n",
    "        array_SVD[i[0]] = global_mean\n",
    "        array_KNN[i[0]]= global_mean\n",
    "        array_NMF[i[0]] = global_mean\n",
    "        array_SlopeOne[i[0]]= global_mean\n",
    "        array_CoClustering[i[0]] = global_mean\n",
    "        count_bad_both+=1\n",
    "print(f\"Done with {count_bad_users} bad users, {count_bad_items} bad items, {count_bad_both} bad both.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traitement des valeurs non estimées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Here we treat the value of -1000'''\n",
    "'''\n",
    "mean_svd = np.mean(array_SVD[array_SVD != -1000])\n",
    "array_SVD[array_SVD==-1000] = mean_svd\n",
    "\n",
    "mean_knn = np.mean(array_KNN[array_KNN != -1000])\n",
    "array_KNN[array_KNN==-1000] = mean_knn\n",
    "\n",
    "mean_nmf = np.mean(array_NMF[array_NMF != -1000])\n",
    "array_NMF[array_NMF==-1000] = mean_nmf\n",
    "\n",
    "mean_slopeone = np.mean(array_SlopeOne[array_SlopeOne != -1000])\n",
    "array_SlopeOne[array_SlopeOne==-1000] = mean_slopeone\n",
    "\n",
    "mean_coclustering = np.mean(array_CoClustering[array_CoClustering != -1000])\n",
    "array_CoClustering[array_CoClustering==-1000] = mean_coclustering\n",
    "'''\n",
    "\n",
    "#apparement plus d'erreur en faisant les -1 la cell d'avant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blending "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Full blending to do\n",
    "Either by \n",
    "-using some model like linear regression\n",
    "-ponderation\n",
    "-something else, can be worked in the blending branch\n",
    "\n",
    "results in final_array\n",
    "'''\n",
    "\n",
    "#here we blend simply by taking the mean of all models\n",
    "tmp = np.concatenate((array_SVD, array_KNN, array_NMF, array_SlopeOne, array_CoClustering), axis=1 )\n",
    "tmp[np.where(tmp>5)]=5\n",
    "tmp[np.where(tmp<1)]=1\n",
    "weights=np.ones(len(tmp[1]))\n",
    "final_array = np.average(tmp, axis=1, weights)\n",
    "final_array = np.rint(final_array)\n",
    "final_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation en submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.Prediction = final_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.drop(columns=[\"user\", \"item\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv(\"Datasets/submission_pipeline.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
