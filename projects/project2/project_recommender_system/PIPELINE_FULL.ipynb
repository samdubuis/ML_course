{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: surprise in /usr/local/lib/python3.6/dist-packages (0.1)\r\n",
      "Requirement already satisfied: scikit-surprise in /usr/local/lib/python3.6/dist-packages (from surprise) (1.1.0)\r\n",
      "Requirement already satisfied: six>=1.10.0 in /home/alix/.local/lib/python3.6/site-packages (from scikit-surprise->surprise) (1.11.0)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-surprise->surprise) (0.14.0)\r\n",
      "Requirement already satisfied: numpy>=1.11.2 in /home/alix/.local/lib/python3.6/site-packages (from scikit-surprise->surprise) (1.15.2)\r\n",
      "Requirement already satisfied: scipy>=1.0.0 in /home/alix/.local/lib/python3.6/site-packages (from scikit-surprise->surprise) (1.1.0)\r\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "!pip3 install surprise\n",
    "from surprise import Reader\n",
    "from surprise import Dataset\n",
    "\n",
    "from surprise import SVD, KNNBasic, NMF, SlopeOne, CoClustering #other knn, randoms, not svdpp because were not doing implicit               # importer ici les algo qu'on testera\n",
    "from surprise import model_selection\n",
    "from surprise import dump\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import RidgeCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(x):\n",
    "    \"\"\"Standardize the original data set.\"\"\"\n",
    "    mean_x = np.mean(x)\n",
    "    x = x - mean_x\n",
    "    std_x = np.std(x)\n",
    "    x = x / std_x\n",
    "    return x, mean_x, std_x\n",
    "\n",
    "def expansion(x,degree=1):\n",
    "    #polynomial expansion\n",
    "    X_ex=x\n",
    "    for deg in range(2, degree):\n",
    "        X_ex = np.c_[X_ex, np.power(x, deg)]\n",
    "    #cross term\n",
    "    for i in range(0, x.shape[1]):\n",
    "        for j in range(i+1, x.shape[1]):\n",
    "            X_ex = np.c_[X_ex, np.multiply(x[:,i],x[:,j])]\n",
    "    return X_ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>r44_c1</td>\n",
       "      <td>4</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>r61_c1</td>\n",
       "      <td>3</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>r67_c1</td>\n",
       "      <td>4</td>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>r72_c1</td>\n",
       "      <td>3</td>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>r86_c1</td>\n",
       "      <td>5</td>\n",
       "      <td>86</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1176947</td>\n",
       "      <td>r9990_c1000</td>\n",
       "      <td>4</td>\n",
       "      <td>9990</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1176948</td>\n",
       "      <td>r9992_c1000</td>\n",
       "      <td>5</td>\n",
       "      <td>9992</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1176949</td>\n",
       "      <td>r9994_c1000</td>\n",
       "      <td>3</td>\n",
       "      <td>9994</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1176950</td>\n",
       "      <td>r9997_c1000</td>\n",
       "      <td>4</td>\n",
       "      <td>9997</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1176951</td>\n",
       "      <td>r10000_c1000</td>\n",
       "      <td>3</td>\n",
       "      <td>10000</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1176952 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Id  Prediction   user  item\n",
       "0              r44_c1           4     44     1\n",
       "1              r61_c1           3     61     1\n",
       "2              r67_c1           4     67     1\n",
       "3              r72_c1           3     72     1\n",
       "4              r86_c1           5     86     1\n",
       "...               ...         ...    ...   ...\n",
       "1176947   r9990_c1000           4   9990  1000\n",
       "1176948   r9992_c1000           5   9992  1000\n",
       "1176949   r9994_c1000           3   9994  1000\n",
       "1176950   r9997_c1000           4   9997  1000\n",
       "1176951  r10000_c1000           3  10000  1000\n",
       "\n",
       "[1176952 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Datasets/data_train.csv\")\n",
    "\n",
    "df[[\"user\", \"item\"]] = df.Id.str.split(\"_\", expand=True)\n",
    "\n",
    "df.user = df.user.str.replace(\"r\", \"\")\n",
    "df.item = df.item.str.replace(\"c\", \"\")\n",
    "\n",
    "#########\n",
    "df2 = pd.read_csv(\"Datasets/sample_submission.csv\")\n",
    "\n",
    "df2[[\"user\", \"item\"]] = df2.Id.str.split(\"_\", expand=True)\n",
    "\n",
    "df2.user = df2.user.str.replace(\"r\", \"\")\n",
    "df2.item = df2.item.str.replace(\"c\", \"\")\n",
    "prev_size=df.shape[0]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading des pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, algo_svd = dump.load(\"dump/dump_SVD\")\n",
    "_, algo_knn = dump.load(\"dump/dump_KNN_basic\")\n",
    "_, algo_nmf = dump.load(\"dump/dump_NMF\")\n",
    "_, algo_SlopeOne = dump.load(\"dump/dump_SlopeOne\")\n",
    "_, algo_coclustering = dump.load(\"dump/dump_CoClustering\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_users(line):\n",
    "    '''Get the userID'''\n",
    "    row, col = line.split(\"_\")\n",
    "    row = row.replace(\"r\", \"\")\n",
    "    return str(row)\n",
    "\n",
    "def get_items(line):\n",
    "    '''get the movieID'''\n",
    "    row, col = line.split(\"_\")\n",
    "    col = col.replace(\"c\", \"\")\n",
    "    return str(col)\n",
    "\n",
    "def to_surprise(data):\n",
    "    '''Move the dataframe from the Id/rating format to the userID/itemID/rating format'''\n",
    "    data['userID'] = data['Id'].apply(get_users)\n",
    "    data['itemID'] = data['Id'].apply(get_items)\n",
    "    data = data.drop('Id', axis=1)\n",
    "    data = data.rename(columns={'Prediction':'rating'})[['userID','itemID','rating']]\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_original = pd.read_csv('Datasets/sample_submission.csv')\n",
    "test = test_original.copy()\n",
    "test = to_surprise(test)\n",
    "test = Dataset.load_from_df(test, reader)\n",
    "test = test.build_full_trainset()\n",
    "test = test.build_testset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "algo_svd.predict(\"37\",\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_SVD=algo_svd.test(test,verbose=False)\n",
    "np.save(\"pred_svd_for_submission\",array_SVD)\n",
    "del array_SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_KNN=algo_knn.test(test)\n",
    "np.save(\"pred_knn_for_submission\",array_KNN)\n",
    "del array_KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_NMF=algo_nmf.test(test)\n",
    "np.save(\"pred_nmf_for_submission\",array_NMF)\n",
    "del array_NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_SlopeOne=algo_SlopeOne.test(test)\n",
    "np.save(\"pred_slopeone_for_submission\",array_SlopeOne)\n",
    "del array_SlopeOne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_CoClustering=algo_coclustering.test(test)\n",
    "np.save(\"pred_coclustering_for_submission\",array_CoClustering)\n",
    "del array_CoClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_SVD=np.load(\"pred_svd_for_submission.npy\")\n",
    "array_KNN=np.load(\"pred_knn_for_submission.npy\")\n",
    "array_NMF=np.load(\"pred_nmf_for_submission.npy\")\n",
    "array_CoClustering=np.load(\"pred_coclustering_for_submission.npy\")\n",
    "array_SlopeOne=np.load(\"pred_slopeone_for_submission.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp=np.vstack([array_SVD[:,3], array_KNN[:,3], array_NMF[:,3], array_SlopeOne[:,3],array_CoClustering[:,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp=np.transpose(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "del array_CoClustering\n",
    "del array_KNN\n",
    "del array_SlopeOne\n",
    "del array_SVD\n",
    "del array_NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contruction of matrix for ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Alix 08/12 replace this shit by the .test() method as above but on the trainset \n",
    "valid_items=df.item.unique()\n",
    "valid_users=df.user.unique()\n",
    "global_mean=df[\"Prediction\"].mean()\n",
    "count_bad_users=0\n",
    "count_bad_items=0\n",
    "count_bad_both=0\n",
    "global_mean=df[\"Prediction\"].mean()\n",
    "\n",
    "for i in df.iterrows():\n",
    "    if i[0]%100000==0:\n",
    "        print(i[0])\n",
    "    if (i[1][\"user\"] in valid_users and i[1][\"item\"] in valid_items):\n",
    "        array_SVD[i[0]] = algo_svd.estimate(int(i[1][2])-1, int(i[1][3])-1)\n",
    "        array_KNN[i[0]]= algo_knn.estimate(int(i[1][2])-1, int(i[1][3])-1)[0]\n",
    "        array_NMF[i[0]] = algo_nmf.estimate(int(i[1][2])-1, int(i[1][3])-1)\n",
    "        array_SlopeOne[i[0]]=algo_slopeone.estimate(int(i[1][2])-1, int(i[1][3])-1)\n",
    "        array_CoClustering[i[0]] = algo_coclustering.estimate(int(i[1][2])-1, int(i[1][3])-1)\n",
    "    elif (i[1][\"user\"] in valid_users):\n",
    "        user_mean=df[df[\"user\"]==i[1][\"user\"]].mean()\n",
    "        array_SVD[i[0]] = user_mean\n",
    "        array_KNN[i[0]]= user_mean\n",
    "        array_NMF[i[0]] = user_mean\n",
    "        array_SlopeOne[i[0]]=user_mean\n",
    "        array_CoClustering[i[0]] = user_mean\n",
    "        count_bad_users+=1\n",
    "    elif (i[1][\"item\"] in valid_items):\n",
    "        item_mean=df[df[\"item\"]==i[1][\"item\"]].mean()\n",
    "        array_SVD[i[0]] = item_mean\n",
    "        array_KNN[i[0]]= item_mean\n",
    "        array_NMF[i[0]] = item_mean\n",
    "        array_SlopeOne[i[0]]= item_mean\n",
    "        array_CoClustering[i[0]] = item_mean\n",
    "        count_bad_items+=1\n",
    "    else:\n",
    "        array_SVD[i[0]] = global_mean\n",
    "        array_KNN[i[0]]= global_mean\n",
    "        array_NMF[i[0]] = global_mean\n",
    "        array_SlopeOne[i[0]]= global_mean\n",
    "        array_CoClustering[i[0]] = global_mean\n",
    "        count_bad_both+=1\n",
    "print(f\"Done with {count_bad_users} bad users, {count_bad_items} bad items, {count_bad_both} bad both.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((array_SVD, array_KNN, array_NMF, array_SlopeOne, array_CoClustering), axis=1)\n",
    "X[np.where(X>5)]=5\n",
    "X[np.where(X<1)]=1\n",
    "np.save(\"full_pred_array_in_range_1_5\",X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"X_untouched = np.load(\"full_pred_array.npy\")\n",
    "print(np.where(X_untouched>5))\n",
    "print(np.where(X_untouched<1))\n",
    "\"\"\"\n",
    "X=np.load(\"full_pred_array_in_range_1_5.npy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_expand = expansion(X,4)\n",
    "del X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 3 4 ... 3 4 3]\n"
     ]
    }
   ],
   "source": [
    "y=np.array(df[\"Prediction\"])\n",
    "print(y)\n",
    "clf=RidgeCV(alphas=np.linspace(10**-5,1,10),cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_expand=standardize(X_expand)[0]\n",
    "clf=clf.fit(X_expand,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.56953879, -1.45354922,  3.72157772,  5.39914987,  2.1460635 ,\n",
       "       -1.76588936,  6.90499038,  1.99524274,  3.15559124, -1.31329082,\n",
       "        0.16726713, -0.61157989, -0.11697398,  0.19776078,  0.07545713,\n",
       "        0.59147389,  0.76999595,  0.04250572, -0.50183058, -0.10444069,\n",
       "       -0.70131696, -0.28465985, -8.25135512,  2.56837642, -0.54938787])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.alpha_\n",
    "clf.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(clf.cv_values_)\n",
    "print(clf.coef_)\n",
    "pred=np.rint(clf.predict(X_expand))\n",
    "print(np.unique(pred,return_counts=True))\n",
    "print(pred)\n",
    "print(y)\n",
    "len(np.where(y-pred==0.0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test= train_test_split(X_expand, test_size=0.2)\n",
    "\n",
    "\n",
    "\n",
    "model = xgb.XGBClassifier()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "print(model.feature_importances_)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"valid_items=df.item.unique()\n",
    "valid_users=df.user.unique()\n",
    "global_mean=df[\"Prediction\"].mean()\n",
    "count_bad_users=0\n",
    "count_bad_items=0\n",
    "count_bad_both=0\n",
    "global_mean=df[\"Prediction\"].mean()\n",
    "\n",
    "for i in df2.iterrows():\n",
    "    if i[0]%100000==0:\n",
    "        print(i[0])\n",
    "    if (i[1][\"user\"] in valid_users and i[1][\"item\"] in valid_items):\n",
    "        array_SVD[i[0]] = algo_svd.estimate(int(i[1][2])-1, int(i[1][3])-1)\n",
    "        array_KNN[i[0]]= algo_knn.estimate(int(i[1][2])-1, int(i[1][3])-1)[0]\n",
    "        array_NMF[i[0]] = algo_nmf.estimate(int(i[1][2])-1, int(i[1][3])-1)\n",
    "        array_SlopeOne[i[0]]=algo_SlopeOne.estimate(int(i[1][2])-1, int(i[1][3])-1)\n",
    "        array_CoClustering[i[0]] = algo_coclustering.estimate(int(i[1][2])-1, int(i[1][3])-1)\n",
    "    elif (i[1][\"user\"] in valid_users):\n",
    "        user_mean=df[df[\"user\"]==i[1][\"user\"]].mean()\n",
    "        array_SVD[i[0]] = user_mean\n",
    "        array_KNN[i[0]]= user_mean\n",
    "        array_NMF[i[0]] = user_mean\n",
    "        array_SlopeOne[i[0]]=user_mean\n",
    "        array_CoClustering[i[0]] = user_mean\n",
    "        count_bad_users+=1\n",
    "    elif (i[1][\"item\"] in valid_items):\n",
    "        item_mean=df[df[\"item\"]==i[1][\"item\"]].mean()\n",
    "        array_SVD[i[0]] = item_mean\n",
    "        array_KNN[i[0]]= item_mean\n",
    "        array_NMF[i[0]] = item_mean\n",
    "        array_SlopeOne[i[0]]= item_mean\n",
    "        array_CoClustering[i[0]] = item_mean\n",
    "        count_bad_items+=1\n",
    "    else:\n",
    "        array_SVD[i[0]] = global_mean\n",
    "        array_KNN[i[0]]= global_mean\n",
    "        array_NMF[i[0]] = global_mean\n",
    "        array_SlopeOne[i[0]]= global_mean\n",
    "        array_CoClustering[i[0]] = global_mean\n",
    "        count_bad_both+=1\n",
    "print(f\"Done with {count_bad_users} bad users, {count_bad_items} bad items, {count_bad_both} bad both.\")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blending "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.28073502 3.34849562 3.44458121 ... 3.92069315 3.27649844 2.75553939]\n"
     ]
    }
   ],
   "source": [
    "#tmp=np.load(\"full_pred_array_in_range_1_5_for_submission.npy\")\n",
    "tmp=standardize(expansion(tmp,4))[0]\n",
    "pred=clf.predict(tmp)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3. 3. 3. ... 4. 3. 3.]\n",
      "(array([2., 3., 4., 5.]), array([  7129, 308056, 858446,   3321]))\n"
     ]
    }
   ],
   "source": [
    "final_array=np.rint(pred)\n",
    "final_array[np.where(final_array>5)]=5\n",
    "final_array[np.where(final_array<1)]=1\n",
    "print(final_array)\n",
    "print(np.unique(final_array, return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation en submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.Prediction = final_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.drop(columns=[\"user\", \"item\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv(\"Datasets/submission_pipeline.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
